# from transformers import AutoProcessor, Llama4ForConditionalGeneration
# import torch

MODEL = "meta-llama/Llama-4-Scout-17B-16E-Instruct"
FOLDER = ""
LOCATION = ""

processor = "a"
model = "b"

messages = []

inputs = 0
outputs = 1

response = "Hello, world."
print(response)
